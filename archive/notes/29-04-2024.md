# Notes - 29th April 2024
(Ask about BEC 
and Transdisciplinary Nature Conservation Summer School)

- **Mixture Model with Missing Data is not reliable (doesn't run very well for large numbers)
- Option to choose the number of clusters in the model:
    - **BIC**: Bayesian Information Criterion
    - **AIC**: Akaike Information Criterion
    - Some sort of cross-validation.
    - Free $K$ as part of the model.
- Variational Bayesian Gaussian Mixture (another alternative): Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. In theory the model could choose not to use some components (may be useful for choosing K).
- Once we have the clusters:
    - https://stats.stackexchange.com/questions/197596/how-to-profile-visualise-and-understand-large-number-of-groups-classes-clusters
    - For the $M$ features in your data, in total and by cluster, create a $(k+1)\cdot M$ matrix, with k=# clusters, then simply divide the cluster values by the grand mean (median, whatever) for each feature, multiply by hundred and round off the decimals. The resulting index is like an IQ score where indices of 80 and less or 120+ are considered (un)representative of that cluster. Really simple but it's useful for quick and dirty insights. Once you have the indices, you can create a heat map of the features that highlight the deviances. Here's a link to an introduction to heat mapping that is fairly clear: http://www.fusioncharts.com/dev/chart-guide/heat-map-chart/introduction.html
- How to impute the data missing data:
    - Possible methods
        - **KNN**
        - **MICE**
        - **EM**
    - Cross-validation (even though we'll end up using the same data at the end). We could simulate having more missing data and predict those values.
- Shuffle traits and see how everything changes.
